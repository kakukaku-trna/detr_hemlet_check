#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized video inference script for DETR-like models.
- Uses proper Resize + Normalize transforms (match resnet50 standard)
- Robust checkpoint loading (handles "module." prefixes)
- Uses postprocessors['bbox'] when available to map boxes to original image size
- Lower confidence threshold (default 0.3)
- Optional mixed precision and optional NMS (disabled by default)
- Simple temporal smoothing option
"""
import argparse
import os
import time
from pathlib import Path

import random
import numpy as np
from PIL import Image
import cv2
import torch
from torchvision import transforms
from torchvision.ops.boxes import batched_nms

# Import your model builder (assumes build_model returns model, criterion, postprocessors)
# from models import build_model  # <-- Uncomment / adapt if your repo layout differs
# For this script to run you must have `build_model` available in models.py
from models import build_model

# ---------------------------
# Utility functions
# ---------------------------

def get_args_parser():
    parser = argparse.ArgumentParser('DETR video inference', add_help=False)
    parser.add_argument('--video', type=str, default="inference_demo/detect_demo/2.mp4", help="Path to input video")
    parser.add_argument('--output_dir', default='./inference_output', help='path where to save results')
    parser.add_argument('--resume', default='/root/detr/data/output/checkpoint.pth', help='Path to checkpoint')
    parser.add_argument('--device', default='cuda', help='device for inference (cuda or cpu)')
    parser.add_argument('--confidence', default=0.3, type=float, help='confidence threshold (lower for higher recall)')
    parser.add_argument('--apply_nms', action='store_true', help='apply NMS after filtering (not necessary for DETR)')
    parser.add_argument('--nms_iou', default=0.5, type=float, help='NMS IoU threshold if apply_nms is set')
    parser.add_argument('--resize_short', default=800, type=int, help='short side resize used in transform')
    parser.add_argument('--amp', action='store_true', help='use mixed precision (torch.cuda.amp) for inference')
    parser.add_argument('--smooth_alpha', default=0.0, type=float, help='temporal smoothing alpha in [0,1], 0=disabled')
    parser.add_argument('--num_queries', default=100, type=int, help='number of queries (for info)')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--dataset_file', default='coco', type=str,
                        help="Dataset type: coco, voc, etc.")
    parser.add_argument('--num_classes', default=3, type=int,
                        help="Number of object classes (91 for COCO)")
    # æ¨¡å‹ç»“æ„å‚æ•°
    parser.add_argument('--hidden_dim', default=256, type=int,
                        help="Transformer éšè—å±‚ç»´åº¦")
    parser.add_argument('--backbone', default='resnet50', type=str,
                        help="backbone name")
    parser.add_argument('--dilation', action='store_true',
                        help="æ˜¯å¦ä½¿ç”¨ç©ºæ´å·ç§¯")
    parser.add_argument('--dropout', default=0.1, type=float)
    parser.add_argument('--nheads', default=8, type=int,
                        help="Transformer æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument('--enc_layers', default=6, type=int,
                        help="Transformer encoder å±‚æ•°")
    parser.add_argument('--dec_layers', default=6, type=int,
                        help="Transformer decoder å±‚æ•°")
    parser.add_argument('--pre_norm', action='store_true')
    parser.add_argument('--masks', action='store_true',
                        help="æ˜¯å¦åˆ†å‰²æ¨¡å¼ï¼ˆæ¨ç†è§†é¢‘æ—¶ä¸éœ€è¦ï¼‰")
    parser.add_argument('--position_embedding', default='sine', type=str,
                        choices=('sine', 'learned', 'v2'),
                        help="Type of positional embedding to use on top of the image features")
    parser.add_argument('--lr_backbone', default=1e-5, type=float,
                        help="backbone å­¦ä¹ ç‡ï¼Œ>0 è¡¨ç¤ºè®­ç»ƒæ—¶æ›´æ–° backbone æƒé‡ï¼Œæ¨ç†å¯éšä¾¿è®¾")
    parser.add_argument('--dim_feedforward', default=2048, type=int,
                        help="Transformer FFN éšè—å±‚ç»´åº¦")
    parser.add_argument('--aux_loss', action='store_true',
                        help="æ˜¯å¦ä½¿ç”¨è§£ç å™¨ä¸­é—´å±‚çš„è¾…åŠ©æŸå¤±ï¼ˆæ¨ç†æ—¶å¯å¼€å¯å…³ï¼‰")
    parser.add_argument('--set_cost_class', default=1, type=float,
                        help="åŒ¹é…ä»£ä»·ï¼šåˆ†ç±»æŸå¤±æƒé‡")
    parser.add_argument('--set_cost_bbox', default=5, type=float,
                        help="åŒ¹é…ä»£ä»·ï¼šL1 æ¡†å›å½’æŸå¤±æƒé‡")
    parser.add_argument('--set_cost_giou', default=2, type=float,
                        help="åŒ¹é…ä»£ä»·ï¼šGIoU æŸå¤±æƒé‡")
    parser.add_argument('--bbox_loss_coef', default=5, type=float,
                        help="åˆ†ç±»æŸå¤±æƒé‡")
    parser.add_argument('--giou_loss_coef', default=2, type=float,
                        help="GIoU æŸå¤±æƒé‡")
    parser.add_argument('--eos_coef', default=0.1, type=float,
                        help="ç©ºç±»ï¼ˆèƒŒæ™¯ï¼‰æŸå¤±æƒé‡")
    return parser





import argparse
import random
import time
from pathlib import Path
import numpy as np
import torch
from models import build_model
import cv2
import torchvision
from torchvision.ops.boxes import batched_nms
import sys
sys.path.append(r"F:\ä¸çŸ¥å\åƒåœ¾æ–‡ä»¶\sort-master")  # å¤–å±‚ç›®å½•
from sort.sort import Sort
# ----------------- å·¥å…·å‡½æ•° -----------------
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    return b

def filter_boxes(scores, boxes, confidence=0.7, apply_nms=True, iou=0.5):
    keep = scores.max(-1).values > confidence
    scores, boxes = scores[keep], boxes[keep]
    if apply_nms:
        top_scores, labels = scores.max(-1)
        keep = batched_nms(boxes, top_scores, labels, iou)
        scores, boxes = scores[keep], boxes[keep]
    return scores, boxes

def plot_one_box(x, img, color=None, label=None, line_thickness=2):
    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1
    color = color if color is not None else [255, 0, 0]
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
    if label:
        tf = max(tl - 1, 1)
        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]
        c2_text = c1[0] + t_size[0], c1[1] - t_size[1] - 3
        cv2.rectangle(img, c1, c2_text, color, -1, cv2.LINE_AA)
        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)

def random_color():
    return [random.randint(0, 255) for _ in range(3)]

# ----------------- ç±»åˆ«è®¾ç½® -----------------
CLASSES = ['N/A', 'helmet', 'no_helmet']
COLORS = {
    'helmet': [0, 255, 0],        # ç»¿è‰²
    'no_helmet': [0, 0, 255],     # çº¢è‰²
    'N/A': [0, 0, 255],       # ç™½è‰²ï¼ˆå¯éšä¾¿ï¼‰
}


# ----------------- è§†é¢‘æ¨ç† -----------------
def main(args):
    device = torch.device(args.device)
    model, _, _ = build_model(args)
    checkpoint = torch.load(args.resume, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    model.eval()
    tracker = Sort(max_age=5, min_hits=1, iou_threshold=0.3)
    already_alerted_ids = set()  # ç”¨äºè®°å½•å·²ç»æŠ¥è­¦çš„ track_id

    image_Totensor = torchvision.transforms.ToTensor()

    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(args.video)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    output_path = args.output_dir + "/output_video1111111.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1  # å¸§è®¡æ•°

        # è½¬æˆTensor
        img_tensor = image_Totensor(frame).unsqueeze(0).to(device)

        # æ¨ç†
        with torch.no_grad():
            outputs = model(img_tensor)
        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].cpu()
        bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0].cpu(), (width, height))
        scores, boxes = filter_boxes(probas, bboxes_scaled)

        scores_np = scores.numpy()
        boxes_np = boxes.numpy()
        # ç”»æ¡†
        for i in range(boxes_np.shape[0]):
            class_id = scores_np[i].argmax()
            raw_label = CLASSES[class_id]
            confidence = scores_np[i].max()

            # å°† "N/A" ç»Ÿä¸€ä¸º "no_helmet"
            if raw_label == "N/A":
                raw_label = "no_helmet"

            x1, y1, x2, y2 = boxes_np[i]
            box_area = (x2 - x1) * (y2 - y1)

            # ------------------------------
            #   ğŸ¯ å¢å¼ºç‰ˆåˆ†ç±»é€»è¾‘
            # ------------------------------
            # 1. å°æ¡†å¿½ç•¥ï¼ˆé¿å…å™ªå£°ï¼‰
            if box_area < 200:  # ä½ å¯ä»¥è°ƒä¸º 200~600
                continue
            # 2. é»˜è®¤æ ‡ç­¾
            label = raw_label

            # 4. ä½ç½®ä¿¡åº¦ä¹Ÿåˆ¤ä¸º no_helmetï¼ˆæ›´ä¿å®ˆï¼‰
            if confidence < 0.45 and raw_label != "helmet":
                label = "no_helmet"

            if raw_label == "helmet" and confidence < 0.85:
                continue  # è·³è¿‡è¿™ä¸ªæ¡†ï¼Œä¸ç”»æ¡†ä¹Ÿä¸æŠ¥è­¦

            # ------------------------------
            #   ğŸ¨ ç”»æ¡†
            # ------------------------------
            text = f"{label} {confidence:.3f}"
            plot_one_box(boxes_np[i], frame, color=COLORS[label], label=text)
        # å‡†å¤‡ SORT è¾“å…¥ï¼šåªè¿½è¸ª no_helmet æˆ– N/A
        dets = []
        for i in range(boxes_np.shape[0]):
            class_id = scores_np[i].argmax()
            raw_label = CLASSES[class_id]

            # å°† N/A å½“ no_helmet
            if raw_label == "N/A":
                raw_label = "no_helmet"

            if raw_label == "no_helmet":
                x1, y1, x2, y2 = boxes_np[i]
                score = scores_np[i].max()
                dets.append([x1, y1, x2, y2, score])
        dets = np.array(dets)

        # âœ… ä¿®æ”¹è¿™é‡Œï¼Œé˜²æ­¢ç©ºæ•°ç»„å¯¼è‡´æŠ¥é”™
        if dets.shape[0] > 0:
            tracks = tracker.update(dets)
        else:
            tracks = np.empty((0, 5))  # è¿”å›ç©ºæ•°ç»„ï¼Œä¿è¯ shape=[0,5] ä¸ SORT ä¸€è‡´# è¿”å› [x1, y1, x2, y2, track_id]

        # ------------ æ¯ 10 å¸§æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦å­˜åœ¨ no_helmet ------------
        # ------------ æ¯ 10 å¸§æ£€æŸ¥ä¸€æ¬¡è­¦æŠ¥ ------------
        # ------------ æ¯ 10 å¸§æ£€æŸ¥ä¸€æ¬¡è­¦æŠ¥ï¼ˆå¸¦è¿½è¸ªå»é‡ï¼‰ ------------
        if frame_count % 10 == 0:
            for track in tracks:
                x1, y1, x2, y2, track_id = track
                track_id = int(track_id)

                if track_id not in already_alerted_ids:
                    # ä¿å­˜æŠ¥è­¦å¸§
                    save_path = os.path.join(args.output_dir, f"alert_frame_{frame_count}_id{track_id}.jpg")
                    cv2.imwrite(save_path, frame)
                    print(
                        f"âš ï¸ å¸§ {frame_count} â€” æœªä½©æˆ´å®‰å…¨å¸½ï¼track_id={track_id}, å·²ä¿å­˜ï¼š{os.path.abspath(save_path)}")

                    # æ ‡è®°ä¸ºå·²æŠ¥è­¦
                    already_alerted_ids.add(track_id)

        out_video.write(frame)

    cap.release()
    out_video.release()
    print(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ï¼š{output_path}")


if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)
